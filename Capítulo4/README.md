### üïµÔ∏è Pr√°ctica 4.0: Fundamentos de Aprendizaje No Supervisado

¬°Bienvenido a la pr√°ctica sobre el aprendizaje no supervisado\! Aqu√≠ exploraremos c√≥mo descubrir patrones ocultos en los datos sin la ayuda de etiquetas.


**Objetivos de la Pr√°ctica** üéØ

  * Comprender las t√©cnicas de **preprocesamiento** para datos no supervisados.
  * Aplicar los principales algoritmos de **clustering** y de **reducci√≥n de dimensionalidad**.
  * Usar m√©todos para la **detecci√≥n de anomal√≠as**.
  * Evaluar y validar los resultados de los modelos no supervisados utilizando m√©tricas clave.

**Duraci√≥n aproximada:**
- 90 minutos.

**Tabla de ayuda:**

Para la ejecuci√≥n del c√≥digo ingresar a https://colab.research.google.com/ 

### **1. Preprocesamiento Avanzado y Estructura de Datos**

En el aprendizaje no supervisado, el preprocesamiento es crucial porque los algoritmos como K-Means se basan en la distancia entre los puntos. Si las caracter√≠sticas tienen escalas muy diferentes, aquellas con valores m√°s grandes dominar√°n la distancia, afectando los resultados.

Las t√©cnicas principales son:

  * **Normalizaci√≥n (Min-Max Scaling)**: Escala los datos a un rango espec√≠fico, generalmente `[0, 1]`.
  * **Estandarizaci√≥n (Standardization)**: Transforma los datos para que tengan una media de `0` y una desviaci√≥n est√°ndar de `1`.

**Ejercicio:**
Estandariza los datos de un conjunto de ejemplo utilizando `StandardScaler` de Scikit-learn.

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs

# Generar datos de ejemplo
X, y = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=2.0)
df_datos = pd.DataFrame(X, columns=['Feature_1', 'Feature_2'])

print("Datos originales:")
print(df_datos.head())
print("-" * 30)

# Estandarizar los datos
scaler = StandardScaler()
df_escalado = pd.DataFrame(scaler.fit_transform(df_datos), columns=df_datos.columns)

print("Datos estandarizados:")
print(df_escalado.head())
```

**Reto:** Normaliza el mismo conjunto de datos `df_datos` usando `MinMaxScaler` y muestra los primeros 5 registros.

```python
# Pista de c√≥digo para el reto:
# Pista: Importa la clase correcta y sigue los mismos pasos que para StandardScaler.
from sklearn.preprocessing import MinMaxScaler
# Tu c√≥digo para instanciar el normalizador
# Tu c√≥digo para normalizar los datos
```

-----

### **2. Algoritmos de Clustering: Fundamentos y Aplicaciones**

El **clustering** agrupa puntos de datos similares en conjuntos (cl√∫steres) sin usar etiquetas. El algoritmo de clustering m√°s popular es **K-Means**, que asigna cada punto al centroide m√°s cercano.

**Ejercicio:**
Aplica el algoritmo K-Means al conjunto de datos estandarizado y visualiza los cl√∫steres resultantes.

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs

# Generar y estandarizar datos
X, y = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=2.0)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar K-Means con k=4
kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')
kmeans.fit(X_scaled)
etiquetas_cluster = kmeans.labels_

# Visualizar los cl√∫steres
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=etiquetas_cluster, cmap='viridis', s=50)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroides')
plt.title('Clustering con K-Means')
plt.xlabel('Eje X (Escalado)')
plt.ylabel('Eje Y (Escalado)')
plt.legend()
plt.show()
```

**Reto:** Elige el n√∫mero de cl√∫steres √≥ptimo para el conjunto de datos de ejemplo utilizando el **m√©todo del codo (*elbow method*)**.

```python
# Pista de c√≥digo para el reto:
# Pista: Entrena K-Means en un bucle for, guardando el atributo '.inertia_'.
inercia = []
for k in range(1, 11):
    # Instancia y entrena KMeans para cada k
    # Almacena la inercia en la lista
    pass # Reemplaza esta l√≠nea con tu c√≥digo
# Luego, grafica los resultados.
```

-----

### **3. Reducci√≥n de Dimensionalidad (PCA y t-SNE)**

La reducci√≥n de dimensionalidad es crucial para visualizar y procesar conjuntos de datos con muchas caracter√≠sticas.

  * **PCA (An√°lisis de Componentes Principales)**: Una t√©cnica lineal que transforma los datos a un nuevo espacio de menor dimensi√≥n, preservando la mayor varianza posible.
  * **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: Una t√©cnica no lineal que es excelente para visualizar datos de alta dimensi√≥n, ya que prioriza la preservaci√≥n de las distancias locales.

**Ejercicio:**
Aplica PCA para reducir la dimensionalidad del conjunto de datos de Iris a 2 componentes.

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# Cargar el dataset de Iris
iris = load_iris()
X = iris.data
y = iris.target

# Aplicar PCA para reducir a 2 componentes
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Visualizar el resultado
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=50)
plt.title('Visualizaci√≥n de Iris con PCA')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()
```

**Reto:** Aplica `t-SNE` al mismo conjunto de datos de Iris y visualiza el resultado. Compara la visualizaci√≥n con la de PCA.

```python
# Pista de c√≥digo para el reto:
# Pista: Importa la clase TSNE y √∫sala de manera similar a PCA.
from sklearn.manifold import TSNE
# Tu c√≥digo para instanciar TSNE
# Tu c√≥digo para aplicar la transformaci√≥n
# Tu c√≥digo para graficar
```

-----

### **4. Detecci√≥n de Anomal√≠as (Isolation Forest)**

La detecci√≥n de anomal√≠as es la identificaci√≥n de eventos o puntos inusuales que no se ajustan a un patr√≥n esperado. `Isolation Forest` es un algoritmo que "a√≠sla" las anomal√≠as en √°rboles aleatorios, ya que los valores at√≠picos son m√°s f√°ciles de separar de la mayor√≠a de los datos.

**Ejercicio:**
Usa `IsolationForest` para detectar y visualizar anomal√≠as en un conjunto de datos simple.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest

# Generar datos normales
rng = np.random.RandomState(42)
X_normal = 0.3 * rng.randn(100, 2)
# Generar anomal√≠as
X_anomalias = rng.uniform(low=-4, high=4, size=(10, 2))
X_completo = np.concatenate([X_normal, X_anomalias])

# Entrenar el modelo
modelo_anomalias = IsolationForest(contamination=0.1, random_state=42)
modelo_anomalias.fit(X_completo)

# Predecir las anomal√≠as
predicciones = modelo_anomalias.predict(X_completo)

# Visualizar los resultados
plt.figure(figsize=(8, 6))
plt.scatter(X_completo[:, 0], X_completo[:, 1], c=predicciones, cmap='bwr', s=50)
plt.title('Detecci√≥n de Anomal√≠as con Isolation Forest')
plt.show()
```

**Reto:** Cambia el par√°metro `contamination` de `0.1` a `0.05` y re-ejecuta el c√≥digo. ¬øQu√© efecto tiene este cambio en la cantidad de anomal√≠as detectadas?

```python
# Pista de c√≥digo para el reto:
# Pista: Solo necesitas cambiar el valor del par√°metro en la instanciaci√≥n.
# modelo_anomalias = IsolationForest(contamination=0.05, ...)
# Tu c√≥digo para entrenar y predecir
```

-----

### **5. Evaluaci√≥n y Validaci√≥n (Coeficiente de Silueta)**

Evaluar modelos de clustering es un desaf√≠o, ya que no hay etiquetas reales. El **Coeficiente de Silueta** mide qu√© tan bien agrupado est√° un punto dentro de su cl√∫ster. Un valor cercano a **1** indica que el punto est√° bien agrupado; cerca de **0** indica que se encuentra en la frontera entre dos cl√∫steres; y cerca de **-1** indica que ha sido asignado al cl√∫ster incorrecto.

**Ejercicio:**
Calcula el Coeficiente de Silueta para el modelo de K-Means del ejercicio 2.

```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_score

# Generar y estandarizar datos
X, y = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=2.0)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar K-Means con k=4
kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')
etiquetas_cluster = kmeans.fit_predict(X_scaled)

# Calcular el Coeficiente de Silueta
silueta_promedio = silhouette_score(X_scaled, etiquetas_cluster)

print(f"Coeficiente de Silueta promedio: {silueta_promedio:.2f}")
```

**Reto:** ¬øC√≥mo cambia el Coeficiente de Silueta si usas 2 cl√∫steres en lugar de 4? Vuelve a calcular la m√©trica con `n_clusters=2` y compara el resultado.

```python
# Pista de c√≥digo para el reto:
# Pista: Instancia un nuevo modelo con el nuevo n√∫mero de cl√∫steres.
# kmeans_2 = KMeans(n_clusters=2, ...)
# Etiqueta de nuevo los datos
# Vuelve a calcular la m√©trica
```
### Resultado esperado
![imagen resultado](../images/Img4.jpg)
