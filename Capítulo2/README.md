He reformateado el texto a un formato Markdown est谩ndar y claro para que el c贸digo se muestre correctamente en GitHub. Los t铆tulos, subt铆tulos, listas y bloques de c贸digo est谩n ahora correctamente estructurados para una f谩cil lectura y copia.

-----

#  Preprocesamiento de Datos y An谩lisis Exploratorio

隆Bienvenido a la segunda pr谩ctica\! Aqu铆 te centrar谩s en los pasos cruciales para preparar los datos antes de construir un modelo de *machine learning*.

-----

##  Pr谩ctica 2.1: Limpieza, Transformaci贸n y *Feature Engineering*

### **Objetivos**

  * Comprender y aplicar t茅cnicas de **limpieza de datos** para manejar valores nulos y at铆picos.
  * Realizar **transformaciones** esenciales como el escalado de datos num茅ricos y la codificaci贸n de variables categ贸ricas.
  * Crear nuevas variables (*features*) a trav茅s del ***Feature Engineering*** para mejorar el rendimiento de los modelos.

### **1. Limpieza de Datos: Nulos y *Outliers***

Antes de analizar los datos, es vital asegurarse de que est茅n limpios. Los **valores nulos** (`NaN`) y los **valores at铆picos** (*outliers*) pueden sesgar los resultados.

#### **Ejercicio 1: Gesti贸n de Valores Nulos**

Identifica los valores nulos en el *DataFrame* y usa la imputaci贸n por la media para rellenarlos.

```python
import pandas as pd
import numpy as np

# Datos de ejemplo con valores nulos
data = {'Edad': [25, 30, np.nan, 45, 30, 45],
        'Ventas': [150, 200, 180, 220, np.nan, 190],
        'Regi贸n': ['Norte', 'Sur', 'Norte', 'Sur', 'Norte', 'Norte']}
df_ejemplo = pd.DataFrame(data)

print("DataFrame con valores nulos:")
print(df_ejemplo)
print("-" * 30)

# Calcular la media de la columna 'Ventas'
media_ventas = df_ejemplo['Ventas'].mean()
print(f"Media de la columna 'Ventas': {media_ventas}")

# Imputar valores nulos con la media
df_ejemplo['Ventas'] = df_ejemplo['Ventas'].fillna(media_ventas)

print("DataFrame despu茅s de la imputaci贸n:")
print(df_ejemplo)
```

**Reto:** En el *DataFrame* anterior, identifica los nulos en la columna `Edad` y usa la **imputaci贸n por la mediana** para rellenarlos. Explica brevemente por qu茅 la mediana puede ser una mejor opci贸n que la media.

```python
# Pista de C贸digo para el Reto:
# Pista 1: El m茅todo .median() te dar谩 la mediana de una columna.
# Pista 2: El m茅todo .fillna() es el mismo que se us贸 para las ventas.
# Pista 3: La mediana es m谩s robusta frente a valores at铆picos.

# Tu c贸digo aqu铆
```

-----

### **2. Transformaci贸n de Datos: Escalado y Codificaci贸n**

Para que los modelos de *machine learning* funcionen correctamente, los datos a menudo deben ser transformados. El **escalado** pone las variables en la misma escala, mientras que la **codificaci贸n** convierte variables categ贸ricas en n煤meros.

#### **Ejercicio 2: Escalado de Datos Num茅ricos**

Usa el `StandardScaler` de Scikit-learn para escalar las columnas `Ventas` y `Edad`.

```python
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Datos de ejemplo
data = {'Edad': [25, 30, 35, 45, 30, 45],
        'Ventas': [150, 200, 180, 220, 250, 190]}
df_transformacion = pd.DataFrame(data)

# Inicializar el escalador
scaler = StandardScaler()

# Escalar las columnas num茅ricas
df_scaled = scaler.fit_transform(df_transformacion[['Edad', 'Ventas']])

# Convertir el resultado a un DataFrame para visualizar
df_scaled = pd.DataFrame(df_scaled, columns=['Edad_escalada', 'Ventas_escaladas'])

print("DataFrame despu茅s del escalado:")
print(df_scaled)
```

**Reto:** Codifica la columna `Regi贸n` usando ***One-Hot Encoding*** para convertir las categor铆as en columnas num茅ricas. Explica por qu茅 esta t茅cnica es 煤til para el *machine learning*.

```python
# Pista de C贸digo para el Reto:
# Pista 1: Pandas tiene una funci贸n muy 煤til para esto: pd.get_dummies().
# Pista 2: La t茅cnica de One-Hot Encoding crea una nueva columna por cada categor铆a.
# Pista 3: Los modelos de ML no pueden trabajar directamente con texto.

# Tu c贸digo aqu铆
```

-----

### **3. *Feature Engineering* B谩sico**

El ***Feature Engineering*** es el proceso de crear nuevas variables a partir de las existentes. Una buena *feature* puede mejorar significativamente el rendimiento del modelo.

#### **Ejercicio 3: Creaci贸n de Variables Derivadas**

Crea una nueva columna llamada `VentaPorEdad` que sea el resultado de dividir `Ventas` entre `Edad`.

```python
import pandas as pd

# DataFrame con datos limpios
data = {'Edad': [25, 30, 35, 45, 30, 45],
        'Ventas': [150, 200, 180, 220, 250, 190]}
df_fe = pd.DataFrame(data)

# Crear la nueva feature
df_fe['VentaPorEdad'] = df_fe['Ventas'] / df_fe['Edad']

print("DataFrame con la nueva variable:")
print(df_fe)
```

**Reto:** A partir de la columna `Edad`, crea una nueva *feature* categ贸rica llamada `GrupoEdad` con las siguientes categor铆as: `'Joven'` (menor a 35), y `'Adulto'` (35 o m谩s).

```python
# Pista de C贸digo para el Reto:
# Pista 1: Puedes usar el m茅todo .apply() de Pandas con una funci贸n lambda.
# Pista 2: El m茅todo .apply() se ejecuta sobre cada elemento de la serie.
# Pista 3: La sintaxis para la funci贸n lambda es "lambda x: ...".

# Tu c贸digo aqu铆
```

-----

### **4. Reto Final de C贸digo: Ciclo de Preprocesamiento Completo** 

**Descripci贸n del Problema:**
Tienes un conjunto de datos desordenado. Tu objetivo es aplicar todo lo aprendido en esta pr谩ctica para prepararlo para un modelo de *machine learning*.

**Tarea:**

1.  **Limpieza:** Imputa los valores nulos de la columna `Puntuacion` con el valor 0.
2.  **Transformaci贸n:** Escala la columna `Puntuacion`.
3.  ***Feature Engineering*:** Crea una nueva variable llamada `Puntuacion_log` aplicando el logaritmo natural (`np.log()`) a la columna `Puntuacion`.

<!-- end list -->

```python
# Pista de C贸digo para el Reto:
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Datos de ejemplo
datos_reto = {'ID_Usuario': [1, 2, 3, 4, 5],
              'Puntuacion': [100, 250, np.nan, 500, 150]}
df_reto = pd.DataFrame(datos_reto)

# Pista 1: Usa .fillna(0) para la imputaci贸n.
# Pista 2: Usa MinMaxScaler() en lugar de StandardScaler() para este reto.
# Pista 3: El logaritmo se aplica a una columna completa.

# Tu c贸digo aqu铆
```

-----

##  Pr谩ctica 2.2: An谩lisis Exploratorio y Preparaci贸n de Datos

### **Objetivos**

  * Realizar un **an谩lisis exploratorio de datos (EDA)** utilizando visualizaciones.
  * Comprender la importancia de dividir los datos en conjuntos de **entrenamiento y prueba**.
  * Conocer el concepto de **validaci贸n cruzada** para evaluar modelos de manera robusta.

### **1. An谩lisis Exploratorio con Visualizaciones**

El **An谩lisis Exploratorio de Datos (EDA)** es un paso clave para entender las caracter铆sticas de un conjunto de datos. Las visualizaciones nos permiten identificar patrones, tendencias y la distribuci贸n de las variables.

#### **Ejercicio:**

Usa un conjunto de datos simple para visualizar la relaci贸n entre `Edad` y `VentaTotal`, diferenciando a los clientes por su `Estado` (Activo/Inactivo).

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Datos de ejemplo
data = {'Edad': [25, 30, 45, 50, 28, 35, 60],
        'VentaTotal': [150, 200, 180, 220, 250, 190, 300],
        'Estado': ['Activo', 'Activo', 'Inactivo', 'Activo', 'Inactivo', 'Activo', 'Inactivo']}
df_exploracion = pd.DataFrame(data)

# Crear un gr谩fico de dispersi贸n para visualizar la relaci贸n
plt.figure(figsize=(8, 5))
sns.scatterplot(x='Edad', y='VentaTotal', hue='Estado', data=df_exploracion, s=100)
plt.title('Venta Total vs. Edad por Estado de Cliente')
plt.xlabel('Edad')
plt.ylabel('Venta Total')
plt.show()
```

**Reto:** Crea un **histograma** que muestre la distribuci贸n de la `VentaTotal` y un **gr谩fico de caja** (*boxplot*) que visualice la distribuci贸n de las ventas para cada `Estado`.

```python
# Pista de C贸digo para el Reto:
# Pista 1: Usa sns.histplot() o plt.hist() para el histograma.
# Pista 2: Usa sns.boxplot() para el gr谩fico de caja.

# Tu c贸digo aqu铆
```

-----

### **2. Separaci贸n *Train/Test* y Validaci贸n Cruzada**

Antes de entrenar un modelo, debemos dividir nuestros datos para evaluar su rendimiento de forma objetiva.

  * **Divisi贸n *Train/Test***: Separa el conjunto de datos en dos partes. El **conjunto de entrenamiento** se usa para que el modelo aprenda, y el **conjunto de prueba** se usa para evaluar su rendimiento en datos que nunca ha visto. Esto previene el sobreajuste (*overfitting*).
  * **Validaci贸n Cruzada (*Cross-Validation*)**: Es una t茅cnica m谩s robusta para evaluar un modelo. En lugar de una sola divisi贸n, el conjunto de datos se divide en `k` particiones (*folds*). El modelo se entrena `k` veces, usando un *fold* diferente como conjunto de prueba en cada iteraci贸n. El rendimiento final es el promedio de todas las evaluaciones. Esto reduce la varianza de la evaluaci贸n.

#### **Ejercicio:**

Divide el *DataFrame* en un conjunto de entrenamiento y uno de prueba usando una proporci贸n de 80/20.

```python
from sklearn.model_selection import train_test_split
import pandas as pd

# Datos de ejemplo
data = {'Edad': [25, 30, 45, 50, 28, 35, 60],
        'VentaTotal': [150, 200, 180, 220, 250, 190, 300],
        'Estado': ['Activo', 'Activo', 'Inactivo', 'Activo', 'Inactivo', 'Activo', 'Inactivo']}
df_exploracion = pd.DataFrame(data)
X = df_exploracion[['Edad', 'VentaTotal']] # Features (variables de entrada)
y = df_exploracion['Estado'] # Target (variable a predecir)

# Dividir los datos 80/20
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Forma del conjunto de entrenamiento (X_train):", X_train.shape)
print("Forma del conjunto de prueba (X_test):", X_test.shape)
```

**Reto:** Realiza una validaci贸n cruzada de 3 *folds* utilizando un clasificador `LogisticRegression` sobre el *DataFrame* `X` e `y` definidos en el ejercicio. Imprime el promedio de la precisi贸n de la validaci贸n cruzada.

```python
# Pista de C贸digo para el Reto:
# Pista 1: Importa cross_val_score y LogisticRegression.
# Pista 2: Define el modelo y luego usa cross_val_score.
# Pista 3: cross_val_score(modelo, X, y, cv=3, scoring='accuracy').

# Tu c贸digo aqu铆
```
